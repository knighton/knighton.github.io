<html>
<head>
    <title>Deep Learning</title>                                                                                                                                                                                                                                                                                             
    <style type="text/css">
table {
    margin-left: auto;
    margin-right: auto;
}
td {
    vertical-align: top;
}
.file_name {
    margin: 10px;
    padding: 5px;
    font-size: 250%;
    font-weight: bold;
    background-color: #ccc;
    font-family: monospace;
}
pre {
    width: 700px;
    margin: 10px;
    padding: 5px;
    background: #eee;
    font-size: 130%;
    font-weight: bold;
}
    </style>
</head>
<body>
    <table>
        <tr>
            <td>
                <div class="file_name" style="background: #dbb">train.py</div>
                <pre style="background: #fdd">
#!/usr/bin/env python

from argparse import ArgumentParser
import datasets
from glob import glob
from keras_util import EarlyTermination, SaveModelsAndTerminateEarly
import networks
import numpy as np
import pipelines
import random


def parse_args():
    ap = ArgumentParser()
    ap.add_argument('--source', type=str, default='fyre')
    ap.add_argument('--samples_per_class', type=int, default=45000)
    ap.add_argument('--network', type=str, default='dense_0_256_6')
    return ap.parse_args()


def get_epoch(f):
    x = f.index('epoch_') + len('epoch_')
    e = f[x:]
    e = e[:e.index('_')]
    return int(e)


def get_last_checkpoint(d):
    ff = glob(d + '/*')
    if not ff:
        return None, 0

    ff.sort()
    f = ff[-1]

    return f, get_epoch(f) + 1


def transform_data((train, val, test), X_tf, y_tf):
    train = (X_tf.fit_transform(train[0]),
             y_tf.fit_transform(train[1]))
    val = (X_tf.transform(val[0]), y_tf.transform(val[1]))
    test = (X_tf.transform(test[0]), y_tf.transform(test[1]))
    return train, val, test


def build_model(data, network_name, start_weights):
    <span style="background: #dcb">X, y = data[0]
    network, num_frontends = \
        networks.make_network(X, y, network_name)</span>

    if start_weights:
        print 'Loading weights from', start_weights
        network.load_weights(start_weights)

    print 'Compiling'
    network.compile(loss='categorical_crossentropy',
                    optimizer='adam')

    return network, num_frontends


def train(data, model, num_frontends, resume_epoch, model_dir):
    cb = SaveModelsAndTerminateEarly()
    cb.set_params(model_dir, resume_epoch)
    X_train, y_train = data[0]
    X_val, y_val = data[1]

    if 1 < num_frontends:
        X_train = [X_train] * num_frontends
        X_val = [X_val] * num_frontends

    try:
        model.fit(
            X_train, y_train, validation_data=(X_val, y_val),
            nb_epoch=10000, batch_size=128, callbacks=[cb],
            show_accuracy=True)
    except EarlyTermination:
        pass


def run(args):
    random.seed(1337)
    np.random.seed(1337)

    <span style="background: #bcd">print 'Loading raw data'
    raw_data = datasets.all_impermium_one_source(
        args.samples_per_class, args.source)</span>

    <span style="background: #6c6">print 'Building transformers'
    X_tf, y_tf = pipelines.json_to_ints2d()</span>

    print 'Transforming data'
    data = transform_data(raw_data, X_tf, y_tf)

    print 'Locating the most recent checkpoint'
    model_dir = 'models/%d_%s' % (
        args.samples_per_class, args.network)
    start_weights, resume_epoch = get_last_checkpoint(model_dir)

    print 'Building model'
    network, num_frontends = \
        build_model(data, args.network, start_weights)

    print 'Training'
    train(data, network, num_frontends, resume_epoch, model_dir)


if __name__ == '__main__':
    run(parse_args())
</pre>
            </td>
            <td>
                <div class="file_name">requirements.txt</div>
                <pre>
git+https://github.com/knighton/camacho@camacho
keras
matplotlib
sklearn
</pre>
                <div class="file_name" style="background: #bcd">datasets.py</div>
                    <pre style="background: #def">
import gzip
import json
import random


def each_json(fn, max_count):
    print 'Pulling samples from file: %s' % fn
    i = 0
    with open(fn) as f:
        for line in f:
            if i == max_count:
                break
            j = json.loads(line)
            yield j
            i += 1
    assert i == max_count


def all_impermium_one_source(
        max_samples_per_class=45000, source='fyre'):
    pairs = []

    fn = '../../make_impermium_spam_data/data/spam_%s.txt' % source
    for j in each_json(fn, max_samples_per_class):
        pair = (j, 'spam')
        pairs.append(pair)

    fn = '../../make_impermium_spam_data/data/ham_%s.txt' % source
    for j in each_json(fn, max_samples_per_class):
        pair = (j, 'ham')
        pairs.append(pair)

    random.shuffle(pairs)

    n = len(pairs)
    train = pairs[:int(n * 0.8)]
    dev = pairs[int(n * 0.8):int(n * 0.9)]
    test = pairs[int(n * 0.9):]

    train = zip(*train)
    dev = zip(*dev)
    test = zip(*test)

    return train, dev, test
</pre>
                <div class="file_name" style="background: #6c6">pipelines.py</div>
<pre style="background: #bfb">
from camacho.base import Transformer
from camacho.pipelines import TransformerPipeline
from camacho.preprocess.sequence.coders import IntCoder
from camacho.preprocess.binarize.embeddings import Word2Vec
from camacho.preprocess.binarize.onehot import AtomBinarizer
import json
import numpy as np


class ExtractFrontBackText(Transformer):
    def __init__(self, length=128):
        self.length = length

    def transform(self, jj):
        n = self.length / 2
        ss = []
        for j in jj:
            s = j['object']['content']
            front = s[:n]
            if len(front) < n:
                front += '\0' * (n - len(front))
            back = s[-n:]
            if len(back) < n:
                back = '\0' * (n - len(back)) + back
            if self.length % 2:
                s = front + '\0' + back
            else:
                s = front + '\0' + back[1:]
            ss.append(s)
        return ss


def json_to_ints2d():
    X = TransformerPipeline([
        ExtractFrontBackText(length=128),
        IntCoder(min_freq=10),
    ])

    y = TransformerPipeline([
        AtomBinarizer(),
    ])

    return X, y
</pre>
                <div class="file_name" style="background: #dcb">networks.py</div>
                <pre style="background: #fed">
from keras.models import Sequential
from keras.layers.advanced_activations import PReLU
from keras.layers.core import Activation, Dense, Dropout, Flatten, \
    TimeDistributedDense
from keras.layers.embeddings import Embedding
from keras.layers.normalization import BatchNormalization
from keras.layers.recurrent import GRU
import numpy as np


def make_dense(X, y, num_layers, width, dropout):
    assert len(X.shape) == 2
    assert len(y.shape) == 2

    vocab_size = np.amax(X) + 1

    print 'Vocab size:', vocab_size

    m = Sequential()
    m.add(Embedding(vocab_size, 8))
    m.add(Dropout(dropout))

    m.add(TimeDistributedDense(8, 64))
    m.add(Flatten())

    m.add(BatchNormalization((64 * X.shape[1],)))
    m.add(PReLU((64 * X.shape[1],)))
    m.add(Dropout(dropout))
    m.add(Dense(64 * X.shape[1], width))

    for i in range(num_layers):
        m.add(BatchNormalization((width,)))
        m.add(PReLU((width,)))
        m.add(Dropout(dropout))
        m.add(Dense(width, width))

    m.add(BatchNormalization((width,)))
    m.add(PReLU((width,)))
    m.add(Dropout(dropout))
    m.add(Dense(width, y.shape[1]))

    m.add(Activation('softmax'))
    return m, 1


def make_network(X, y, name):
    ss = name.split('_')
    kind = ss[0]
    if kind == 'dense':
        num_layers, width, dropout = map(int, ss[1:])
        return make_dense(X, y, num_layers, width, dropout)
    else:
        assert False
</pre>
            </td>
        </tr>
    </table>
</body>
</html>
