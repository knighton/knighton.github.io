<html>
<head>
    <title>Deep Learning</title>                                                                                                                                                                                                                                                                                             
    <meta charset="UTF-8">
    <style type="text/css">
a {
    text-decoration: none;
}
td {
    vertical-align: top;
}
pre {
    width: 800px;
    font-size: 120%;
}
.page {
    margin-left: auto;
    margin-right: auto;
    width: 800px;
}
    </style>
</head>
<body>
    <div class="page" style="margin-top: 100px">
        <h1>Complete example: predicting IMDb ratings</h1>
    </div>
    <center>
        <table>
            <tr>
                <td>
                    <pre>
from __future__ import absolute_import

import numpy as np
np.random.seed(1337)

from keras.preprocessing import sequence
from keras.optimizers import SGD, RMSprop, Adagrad
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import LSTM, GRU
from keras.datasets import imdb


def main():
    max_vocab_size = 20000
    max_seq_len = 100
    batch_size = 32
    num_epochs = 10

    (X_train, y_train), (X_val, y_val) = \
        imdb.load_data(nb_words=max_vocab_size, test_split=0.2)

    X_train = sequence.pad_sequences(X_train, maxlen=max_seq_len)
    X_val = sequence.pad_sequences(X_val, maxlen=max_seq_len)

    vocab_size = max(np.amax(X_train), np.amax(X_val)) + 1

    print 'Dimensions:', X_train.shape, vocab_size

    model = Sequential()
    model.add(Embedding(vocab_size, 128))
    model.add(Dropout(0.8))
    model.add(LSTM(128, 128))
    model.add(Dropout(0.8))
    model.add(Dense(128, 1))
    model.add(Activation('sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer='adam',
                  class_mode='binary')

    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=num_epochs,
              validation_data=(X_val, y_val), show_accuracy=True)

    score, acc = model.evaluate(
        X_val, y_val, batch_size=batch_size, show_accuracy=True)
    print 'Test score:', score
    print 'Test accuracy:', acc


if __name__ == '__main__':
    main()
</pre>
                </td>
                <td>
                    <pre>
Using gpu device 0: GeForce GTX 860M (CNMeM is disabled)
Dimensions: (20000, 100) 19999
Train on 20000 samples, validate on 5000 samples
Epoch 0
20000/20000 [==============================] - 54s
- loss: 0.6888 - acc: 0.5362 - val_loss: 0.6296 - val_acc: 0.7002
Epoch 1
20000/20000 [==============================] - 56s
- loss: 0.6692 - acc: 0.6035 - val_loss: 0.6654 - val_acc: 0.6070
Epoch 2
20000/20000 [==============================] - 56s
- loss: 0.6101 - acc: 0.6803 - val_loss: 0.5264 - val_acc: 0.7668
Epoch 3
20000/20000 [==============================] - 56s
- loss: 0.6356 - acc: 0.6355 - val_loss: 0.9222 - val_acc: 0.4944
Epoch 4
20000/20000 [==============================] - 57s
- loss: 0.5580 - acc: 0.7452 - val_loss: 0.4828 - val_acc: 0.8158
Epoch 5
20000/20000 [==============================] - 56s
- loss: 0.5099 - acc: 0.7839 - val_loss: 0.5372 - val_acc: 0.6976
Epoch 6
20000/20000 [==============================] - 56s
- loss: 0.4299 - acc: 0.8278 - val_loss: 0.4324 - val_acc: 0.8196
Epoch 7
20000/20000 [==============================] - 55s
- loss: 0.3749 - acc: 0.8542 - val_loss: 0.3981 - val_acc: 0.8280
Epoch 8
20000/20000 [==============================] - 56s
- loss: 0.3256 - acc: 0.8756 - val_loss: 0.3681 - val_acc: 0.8350
Epoch 9
20000/20000 [==============================] - 55s
- loss: 0.2937 - acc: 0.8898 - val_loss: 0.3799 - val_acc: 0.8402
5000/5000 [==============================] - 3s     
Test score: 0.379942858601
Test accuracy: 0.8402
</pre>
                </td>
            </tr>
        </table>
    </center>
</body>
</html>
